# RAG (Retrieval Augmented Generation) using Llama 2 

The above project employs the Llama2 Large language model as a query engine, enhancing its capabilities by accessing additional knowledge from documents. This enables the LLM to search for relevant information within the provided documents. Essentially, it facilitates interacting with your PDF files by leveraging frameworks such as "langchain" and "Llamaindex," thereby supplementing the LLM with extra knowledge. The system can be run locally and is suitable for private conversations in diverse situations where data needs to be isolated from the internet.

The provided "Llama2 RAG.py" is a Streamlit app that can be launched using the command `streamlit run "Llama2 RAG.py"`.

LLM : `meta-llama/Llama-2-7b-chat-hf`.
